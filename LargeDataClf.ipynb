{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Tue Jul 31 22:44:51 2018\n",
    "\n",
    "For large data-set, Single hidden layer NN with dropout regularizer of 0.5/0.8\n",
    "\n",
    "@author: Shasha\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import bcolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_nn_model():\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size + 1, 32, input_length=max_len),\n",
    "        Flatten(),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dropout(0.8),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Manual model Evaluation with K-fold\n",
    "\n",
    "def cv_evaluate_nn_model(build_fn, X, y, nb_epoch=5, n_splits=5, batch_size=64, **kwargs):\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "    model = KerasClassifier(build_fn=build_fn, nb_epoch=nb_epoch, batch_size=batch_size, verbose=1)\n",
    "    results = cross_val_score(model, X, y, cv=kfold)\n",
    "    print('\\nModel average accuracy: {:.2f}'.format(results.mean()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_big = \"./dataset/training-data-large.txt\"\n",
    "test_big = \"./dataset/test-data-large.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#put the data in to a data frame and divide it in to Sample vs Label\n",
    "train_big_df = pd.read_csv(train_big, sep = '\\t', names = [\"Label\", \"Sample\"])\n",
    "\n",
    "train_big_df_sample_len = train_big_df.Sample.str.split(\",\").apply(len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y = np.array(train_big_df.Sample), np.array(train_big_df.Label)\n",
    "\n",
    "#print (X_train[:3])\n",
    "\n",
    "test_big_df = pd.read_csv(test_big, sep = ' ', names = [\"Sample\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_big_df_sample_len = test_big_df.Sample.str.split(\",\").apply(len)\n",
    "\n",
    "X_test = np.array(test_big_df.Sample)\n",
    "\n",
    "# concatenate both train, test set to build the vocabulary\n",
    "X_all = np.concatenate((X_train, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tknzr = Tokenizer(lower = False, split = ',')\n",
    "tknzr.fit_on_texts(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000,)\n",
      "Size of vocab :  67311\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape)\n",
    "\n",
    "vocab_size = len(tknzr.word_counts)\n",
    "print (\"Size of vocab : \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 400) (100000, 400)\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction\n",
    "\n",
    "#It is too costly to transfer all vocab into sequence, so, limiting to 20K\n",
    "max_features = 20000\n",
    "max_len = 400\n",
    "tknzr = Tokenizer(num_words=max_features, lower=False, split=',')\n",
    "tknzr.fit_on_texts(X_all)\n",
    "\n",
    "X_TrainSeqs = tknzr.texts_to_sequences(X_train)\n",
    "X_TestSeqs = tknzr.texts_to_sequences(X_test)\n",
    "\n",
    "X_TrainSeqs = sequence.pad_sequences(X_TrainSeqs, maxlen=max_len)\n",
    "X_TestSeqs = sequence.pad_sequences(X_TestSeqs, maxlen=max_len)\n",
    "\n",
    "print (X_TrainSeqs.shape, X_TestSeqs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saving the data to a file to load quickly in to memory\n",
    "def save_array(fname, arr):\n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "\n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array('./dataset/train_large.dat', X_TrainSeqs)\n",
    "save_array('./dataset/test_large.dat', X_TestSeqs)\n",
    "save_array('./dataset/y_train.dat', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_TrainSeqs = load_array('./dataset/train_large.dat')\n",
    "y = load_array('./dataset/y_train.dat')\n",
    "X_TestSeqs = load_array('./dataset/test_large.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For this large data doing cross-validation would be very time consuming. So here I split the data to training set and validation set to test the models.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_TrainSeqs, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/5\n",
      "800000/800000 [==============================] - 948s 1ms/step - loss: 0.2688 - acc: 0.8890 - val_loss: 0.2352 - val_acc: 0.9021\n",
      "Epoch 2/5\n",
      "800000/800000 [==============================] - 934s 1ms/step - loss: 0.2448 - acc: 0.9015 - val_loss: 0.2347 - val_acc: 0.9036\n",
      "Epoch 3/5\n",
      "800000/800000 [==============================] - 902s 1ms/step - loss: 0.2352 - acc: 0.9058 - val_loss: 0.2351 - val_acc: 0.9043\n",
      "Epoch 4/5\n",
      "800000/800000 [==============================] - 901s 1ms/step - loss: 0.2269 - acc: 0.9091 - val_loss: 0.2384 - val_acc: 0.9026\n",
      "Epoch 5/5\n",
      "800000/800000 [==============================] - 913s 1ms/step - loss: 0.2185 - acc: 0.9117 - val_loss: 0.2451 - val_acc: 0.9019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f11feb7c208>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Initialization & Training\n",
    "model = single_nn_model()\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64) # manual validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_TestSeqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9264522], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
